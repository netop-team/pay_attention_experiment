{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, \\\n",
    "  BitsAndBytesConfig, GPTQConfig\n",
    "import os\n",
    "\n",
    "while \"notebooks\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.utils import rotate_half, apply_rotary_pos_emb, repeat_kv, get_context_length, get_generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc785b861114c0cb82caf79db5971ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33da0aacfb5f4e709b2a77d716aaef56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8b3d945c854594b9c9db15be7960e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabca74603bf44bfb85976d2c486c550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20316f14fa34aff96c8afc0827a9e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16e799a7b934138a09c041e005afaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7f0569895945388cc9772dfe60b88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0537ff178144b2a8fd9856a0c243b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda820189be44954b8d5622569e05c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6367ddcf4924aa687823472722a626d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fc8921a2d34e5ab82d6b447cadf946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e66aaea9f4454c845a3d4fd54d3708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc18413f658c474cacee01be67eb88b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#     cache_dir = \"/Data\"    \n",
    "# )\n",
    "\n",
    "# tunned_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#     quantization_config = quantization_config,\n",
    "#     device_map=\"auto\",\n",
    "#     attn_implementation=\"eager\",\n",
    "#     cache_dir = \"/Data\" \n",
    "# )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    cache_dir = \"/Data\"    \n",
    ")\n",
    "\n",
    "tunned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    quantization_config = quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    cache_dir = \"/Data\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "df = load_dataset(\"stas/openwebtext-10k\", cache_dir=\"/Data\")['train'].to_pandas()\n",
    "df[\"text_len\"] = df[\"text\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_ATTENTION = None\n",
    "instruction = None\n",
    "tokens = None\n",
    "\n",
    "def hook(module, input, output):\n",
    "\n",
    "    if input[0].shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    specific_token_ids = tokenizer(\n",
    "        instruction,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "\n",
    "    mask = torch.isin(tokens['input_ids'], specific_token_ids)\n",
    "\n",
    "    instruction_size = specific_token_ids[1:].size(1)\n",
    "    token_index_in_text = torch.nonzero(mask.squeeze())\\\n",
    "        [2:instruction_size]\\\n",
    "        .squeeze()\n",
    "\n",
    "    hidden_states = input[0]\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "    # qkT, value_states = helper\\\n",
    "    #     .get_attention_before_softmax(module, input, output)\n",
    "    attention_mask = deepcopy(input[1])\n",
    "    position_ids = input[2]\n",
    "    past_key_value = input[3]\n",
    "    cache_position = input[6]\n",
    "\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    # print(attention_mask.max())\n",
    "\n",
    "    attention_mask[:,:,:,token_index_in_text] += DELTA_ATTENTION\n",
    "\n",
    "    query_states = module.q_proj(hidden_states)\n",
    "    key_states = module.k_proj(hidden_states)\n",
    "    value_states = module.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, module.num_heads, module.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = module.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    key_states = repeat_kv(key_states, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(module.head_dim)\n",
    "\n",
    "    if attention_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=module.attention_dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "    if attn_output.size() != (bsz, module.num_heads, q_len, module.head_dim):\n",
    "        raise ValueError(\n",
    "            f\"`attn_output` should be of size {(bsz, module.num_heads, q_len, module.head_dim)}, but is\"\n",
    "            f\" {attn_output.size()}\"\n",
    "        )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    attn_output = attn_output.view(bsz, q_len, -1)\n",
    "    attn_output = module.o_proj(attn_output)\n",
    "\n",
    "    assert (attn_output.shape == output[0].shape)\n",
    "    assert (attn_output!=output[0]).any()\n",
    "\n",
    "    return attn_output, None, output[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module_name, module in tunned_model.named_modules():\n",
    "    \n",
    "#     if module_name.endswith(\"0.self_attn\"):\n",
    "#         module.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Summarize in french\"\n",
    "df[\"context_length\"] = (instruction + '\\n' +df[\"text\"])\\\n",
    "    .apply(lambda x: get_context_length(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = df.query(\"context_length > 2500 & context_length < 3000\")\\\n",
    "    .sample(5, random_state = 42)\n",
    "\n",
    "instructions = [\n",
    "    \"Summarize in french: \",\n",
    "    \"Important: Summarize in french: \",\n",
    "    \"You must summarize the following text in french: \"\n",
    "]\n",
    "\n",
    "new_samples = []\n",
    "for instruction in instructions:\n",
    "    new_samples_df = samples_df.copy()\n",
    "\n",
    "    new_samples_df[\"text\"] = instruction + \" \\n \" + new_samples_df[\"text\"]\n",
    "    new_samples_df[\"instruction\"] = instruction\n",
    "    new_samples.append(new_samples_df)\n",
    "\n",
    "new_samples_df = pd.concat(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "5 examples, 3 instructions, 50 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for idx, row in samples_df.iterrows():\n",
    "    results[idx] = {}\n",
    "    results[idx][\"base_text\"] = row[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                generating text...\n",
      "                sample idx = 3\n",
      "                context_length = torch.Size([1, 2691])\n",
      "                instruction = Summarize in french: \n",
      "                generation_epoch = 0\n",
      "                last generated text = \n",
      "\n",
      "Summarize in french:  \n",
      " NV Energy fights to keep rooftop solar from cutting into its profit\n",
      "\n",
      "The future of solar energy in Nevada is at stake in a furious battle that likely won’t be resolved as the 2015 state legislative session nears an end next month.\n",
      "\n",
      "Solar advocates, Nevada businesses and solar industry reps are pushing for more rooftop solar, saying it’s unfair to force consumers to remain chained to the grid and warning that the state could lose thousands of jobs if it doesn’t adapt. State utility NV Energy claims more household solar means increased prices for traditional customers who can’t or won’t install solar panels on their houses or businesses.\n",
      "\n",
      "It comes down to money for both sides It’s all about dollars for both the solar industry and NV Energy. The utility says ratepayers will be charged an additional $8 million for every percentage point the net metering cap increases. Rooftop solar customers receive a credit worth about 7 cents per kilowatt-hour for powering their homes and the grid with solar electricity. That credit is an incentive to go solar, but it’s also a means for consuming less power from NV Energy, biting into the company’s profit. The solar industry says a tariff and locked-in cap rate will kill the majority of the 6,000 jobs the industry brought to Nevada over the past five years and will limit consumer choice. Early in the legislative session, NV Energy unloaded a team of lobbyists to squelch any attempt to raise the cap. Solar followed with its own lobbying effort, congregating with a consortium of gaming and tech interests. The battle heated up after a bill draft to raise the cap to 10 percent died without a single public hearing or vote. Solar advocates met with lawmakers and the governor — whose outside advisers lobby for NV Energy — but had little success. Now, as time winds down in the session, only one solution is on the table — a punt. Republican Sen. Patricia Farley’s amendment to a building codes bill would allow the Public Utilities Commission to raise the solar cap and to impose up to three tariffs on net metering customers. The eleventh-hour measure was the only way to save the solar industry this session, Farley said. “It gave the solar industry a vehicle to start a discussion,” Farley said. The amendment cleared the Senate and is moving through the Assembly. The compromise is not ideal for companies such as SolarCity and Sunrun, which lease solar panels to customers who participate in net metering. Industry officials say proposed fees could hurt business by discouraging people from participating in a net metering program. Rooftop customers — who pay bills to both the utility and solar companies — pay about 20 percent less for solar than conventional energy, and the fees, industry leaders say, could bite into their cost savings. Adding fees and restricting the cap would be a big win for Berkshire Hathaway Energy and one of its few net metering successes nationally. Berkshire failed to impose caps in Utah and Washington. Arizona instituted a $5 to $7 net metering charge for homeowners. A fee is pending in Wisconsin. Colorado has no cap and no fees. In other words, utility companies in more than 40 states have unsuccessfully fought to eliminate net metering or impose fees.\n",
      "\n",
      "Much of the fight revolves around Nevada’s cap on net metering, an arrangement by which people with rooftop solar can sell extra power they generate back to the grid. Nevada is likely to hit its limit as early as this summer, solar advocates say, which will make it less advantageous for homeowners to tap the enormous solar energy potential of Southern Nevada.\n",
      "\n",
      "The Legislature seems to have sided with NV Energy. On May 17, it passed a solar bill that failed to raise the cap but gave Nevada’s regulatory Public Utilities Commission the ability to levy new fees on net metering customers who come online after the cap is hit. The new fees seem intended to protect NV Energy’s income from what the company has characterized as an unfair subsidy at the expense of nonsolar ratepayers.\n",
      "\n",
      "While NV Energy, owned by Warren Buffett’s Berkshire Hathaway, battles to keep the cap in place, it’s also fighting on another front. A consortium of casinos and businesses is looking to leave NV Energy’s grid and start generating their own power, saying they’re being placed at a competitive disadvantage because they’re paying more for energy than their business rivals in nearby states. The state Public Utilities Commission has said it would charge hefty fees — $27 million in the case of Las Vegas data center Switch — to let industrial ratepayers leave the system.\n",
      "\n",
      "Meanwhile, the utility is facing another threat in the form of technological advances. Tesla’s Powerwall unit, a relatively cheap storage battery that can charge up on solar power, can help business operators and homeowners reduce their reliance on the grid — or, for the very wealthy, leave it altogether.\n",
      "\n",
      "How the regulated monopoly came to be\n",
      "\n",
      "In exchange for building power plants, power lines, distribution networks and maintaining electrical systems, Nevada, like many states, gives public utilities an authorized rate of return. Here, that rate is about 8 percent, authorized by the Nevada Public Utilities Commission. NV Energy’s net income in 2014 was about $354 million, according to Berkshire Hathaway Energy’s SEC filings.\n",
      "\n",
      "NV Energy did not respond to a request for comment on this story.\n",
      "\n",
      "Giving a utility a regulated monopoly over generating and providing power is a compromise. The utility gets a guaranteed profit and in return gives access to everyone who needs it and ensures capacity for all users. It’s the commission that holds the utilities to the bargain, said Stephen Brown, director of the Center for Business and Economic Research at UNLV.\n",
      "\n",
      "“The utility doesn’t have an incentive to operate in the community interest,” Brown said. “That doesn’t mean they don’t, but that’s not their economic incentive. We’re relying on the utility commission to make sure that the utility operates in the public interest.”\n",
      "\n",
      "More rooftop solar production means more competition for NV Energy.\n",
      "\n",
      "The way competition disrupts the energy industry parallels the shift in the telecommunications industry, said Steven Weissman, director of the energy program at UC Berkeley’s Center for Law, Energy and the Environment.\n",
      "\n",
      "“It started with one monopoly utility and a black rotary dial phone in everybody’s home,” said Weissman, referring to AT&T and its monopoly on the U.S. telephone system until its breakup in 1984.\n",
      "\n",
      "By 1996, Weissman said, Congress forced companies to provide competitors access to infrastructure. And the emergence of mobile phone technology made the fight over access to landline infrastructure obsolete.\n",
      "\n",
      "“Now you have a whole generation of people who decide not to get a landline,” Weissman said. “If the phone companies were able to gain anything by resisting opening their networks to competitive providers, it was something of only limited duration. They didn’t create something that preserved their business model long-term.”\n",
      "\n",
      "The way AT&T and its descendents adapted to the loss of their monopoly was to spread into the broadband and mobile sectors, but big electric utilities have been comparatively slow to adapt to competition from new ways of producing power.\n",
      "\n",
      "“What utilities are doing is instinctively looking for ways to take this pesky new technology and bat it away,” Weissman said.\n",
      "\n",
      "Some companies want to produce their own power, but quitting the grid comes at a cost\n",
      "\n",
      "A group of Nevada companies wants to break from NV Energy and stop paying the utility for energy. Instead, the companies want to start generating and purchasing their own power and quit the grid.\n",
      "\n",
      "The group calls itself the Nevada Coalition to Protect Ratepayers and includes Las Vegas Sands and Wynn Resorts, solar companies SolarCity and Sunrun, and Switch.\n",
      "\n",
      "The utilities commission ruled this month that Switch would have to pay $27 million to leave the grid. Switch has asserted it should pay about $18 million.\n",
      "\n",
      "Borenstein said exit fees weren’t unjustified.\n",
      "\n",
      "“I’m sympathetic to the Public Utilities Commission’s view,” Borenstein said. “I would be suspicious of numbers utilities put out, but I don’t think it should be free for customers to just walk away … (They) built the grid to support customers, and there’s all these sunk costs. There may be stranded assets for which costs have to be recovered. When you leave, you have to bear some of those costs.”\n",
      "\n",
      "Yet the combination of limits on net metering programs and high exit fees seems to leave companies squeezed in the middle. Switch has said its energy costs in Nevada are 30 percent higher than competitors’ in nearby states.\n",
      "\n",
      "Some energy companies elsewhere are adapting to new technology and demands for clean energy and more distributed generation. In California, public utility Southern California Edison is testing how to integrate Tesla Powerwall users, both residential and commercial, with its grid. The utility is performing test runs with a small number of Powerwall users to see if the batteries can, in aggregate, be helpful to Edison’s grid needs.\n",
      "\n",
      "“The idea would be: How could a residential storage unit be used to help the grid?” said Kevin Payne, the utility’s senior vice president for customer service. “We could take power (from battery units) when necessary or inject power when it would be helpful to do that.”\n",
      "\n",
      "Payne said the ability to control a customer’s energy requirements or regulate the way customers pump power back into the grid could be a significant resource for the utility if battery storage users increase.\n",
      "\n",
      "In contrast to NV Energy’s resistance to distributed generation, Payne said Southern California Edison is adapting its vision for its power grid to incorporate new technological advances its customers might use.\n",
      "\n",
      "“The grid of the future is going to need to be upgraded and modernized,” Payne said. “Today … power flows from the top to the bottom. Going forward, the grid is going to have different characteristics: generation, solar or other, batteries, demand response. It’s going to require upgrades to the grid to see what’s happening and manage the two-way flow of power.”\n",
      "\n",
      "Arguments for and against legislative changes to solar\n",
      "\n",
      "Some say an increase in rooftop solar production would cause the traditional grid to collapse, others say solar would help meet power needs and help the state reach alternative energy mandates.\n",
      "\n",
      "Senate Bill 374, passed May 17, states that once the net metering cap is hit, new net metering customers will have to pay an additional tariff, to be determined by the Nevada Public Utilities Commission. That means anyone who installs a solar system on a roof after the cap is hit will pay higher rates to use and sell solar energy than net metering customers do now, though how much higher remains unclear.\n",
      "\n",
      "A bill proposed this year to raise the cap from 3 percent to 10 percent never passed. State Sen. Patricia Farley discussed an amendment to SB374 that would pass authority over the cap to the utilities commission, but the amendment wasn’t included in the final version of the bill.\n",
      "\n",
      "Solar industry representatives say the cap must be raised to allow for consumer choice and more industry jobs.\n",
      "\n",
      "“(People’s) consumer choices are driving the growth of a home-grown industry,” said Will Craven, a spokesperson for SolarCity, a solar power system provider and installer. “Rooftop solar jobs by definition must happen in-state.”\n",
      "\n",
      "Several solar advocates point to a study commissioned by the Nevada Public Utilities Commission as proving net metering benefits all customers — those who generate energy and traditional customers.\n",
      "\n",
      "What the state’s study, released last year, actually said was that it’s probably a wash. Net metering probably won’t ultimately cost non-participants more. Distributed generation may be more expensive than building large utility-scale solar plants, but Nevada is required to source 25 percent of its power from renewable sources by 2025, and power from net metering customers may offset the cost of buying renewable power or building more renewable energy plants.\n",
      "\n",
      "Many experts question whether distributed generation is the most cost-effective route for the state to invest in clean energy. Severin Borenstein, a University of California, Berkeley economist who specializes in energy regulation and energy markets, said neither solar industry advocates nor the public utility are being honest about real costs.\n",
      "\n",
      "With distributed generation, Borenstein said, “You lose the economies of scale. And the economies of scale are really large. The economics overall pretty clearly favor grid-scale generation, both wind and solar.”\n",
      "\n",
      "Borenstein said the way net metering is structured is indeed a subsidy.\n",
      "\n",
      "“You’re basically giving them (net metering customers) retail price credit for putting power into the grid,” Borenstein said. “If (the credit for power) were at wholesale rates, it wouldn’t be a subsidy.”\n",
      "\n",
      "Proper rate design — crafting fees to reflect the true costs and benefits of individual solar power generation — is key to fairness, Borenstein said.\n",
      "\n",
      "“Utilities say if you keep installing solar, the grid’s going to collapse and we’re going to go out of business,” Borenstein said. “There really isn’t much chance of that, and we should be a having a discussion about that, whether that’s the best way to put in renewables. Instead, you get politicians who are either boosting utilities or playing to the residential photovoltaic advocates with all this free consumer choice stuff. It’s not true consumer choice if you can just fall back on the grid and the rates don’t reflect the cost.”\n",
      "\n",
      "But in the fight between the utility and solar industry advocates, experts say, a real public discussion of the costs of distributed generation versus utility-scale clean power from solar and wind plants is being lost.\n",
      "\n",
      "What is net metering?\n",
      "\n",
      "The solar energy policy fight in Nevada revolves around a net metering cap, a limit on the amount of solar power that can be bought back from people or institutions with renewable energy systems.\n",
      "\n",
      "If a home or business generates more power from the sun than it uses in a month, NV Energy will buy the extra at retail power rates and give the customer a credit, the net of their power usage and power production. That means a homeowner with solar panels may be able to run his or her house largely on solar energy during the day and resell what he or she doesn’t use to the grid, seeing real reductions in energy costs.\n",
      "\n",
      "But there’s a limit on the amount of net metering the state allows, and solar advocates and solar industry companies say Nevada will hit the existing cap this year, perhaps as early as late summer. The cap is set at 3 percent of the utility’s peak capacity, or 225 megawatts.<|eot_id|><|start_header_id|>assistant\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:09<00:38,  3.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m        generating text...\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m        sample idx = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[0;32m---> 48\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtunned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attention_mask = tokens['attention_mask'].to(\"cuda\"),\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# temperature = 1.\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m    generated text : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;250m \u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/generation/utils.py:1909\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1902\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1903\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1904\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1906\u001b[0m     )\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1909\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1923\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1924\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1925\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/generation/utils.py:2646\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2646\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1166\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1163\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:970\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    959\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    960\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    961\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         cache_position,\n\u001b[1;32m    968\u001b[0m     )\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:161\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:356\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m    347\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    348\u001b[0m             module,\n\u001b[1;32m    349\u001b[0m             name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    354\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/utils/operations.py:186\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 186\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/utils/operations.py:187\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    186\u001b[0m         {\n\u001b[0;32m--> 187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/utils/operations.py:135\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_to_device\u001b[39m(tensor, device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m        The same data structure as `tensor` with all tensors sent to the proper device.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tensor(tensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;66;03m# `torch.Tensor.to(\"npu\")` could not find context when called for the first time (see this [issue](https://gitee.com/ascend/pytorch/issues/I8KECW?from=project-issue)).\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DELTA_ATTENTION = 0\n",
    "n_times_generation = 50\n",
    "decoded = None\n",
    "for generation_epoch in range(n_times_generation):\n",
    "    count = 0\n",
    "\n",
    "    t = tqdm(enumerate(new_samples_df.iterrows()), total = len(new_samples_df))\n",
    "    for i, (idx, row) in t:\n",
    "\n",
    "        prompt = row[\"text\"]\n",
    "        instruction = row['instruction']\n",
    "        message = [ {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        template = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize= False\n",
    "        )\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            template,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        if decoded is not None:\n",
    "\n",
    "            print(f'''\n",
    "                generating text...\n",
    "                sample idx = {i}\n",
    "                context_length = {tokens['input_ids'].shape}\n",
    "                instruction = {instruction}\n",
    "                generation_epoch = {generation_epoch}\n",
    "                last generated text = {decoded[0].split(\"<|end_header_id|>\") [1]}\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(f'''\n",
    "                generating text...\n",
    "                sample idx = {i}\n",
    "                context_length = {tokens['input_ids'].shape}\n",
    "                instruction = {instruction}\n",
    "                generation_epoch = {generation_epoch}\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        generated_ids = tunned_model.generate(\n",
    "            tokens['input_ids'].to('cuda'),\n",
    "            # attention_mask = tokens['attention_mask'].to(\"cuda\"),\n",
    "            max_new_tokens = 30,\n",
    "            do_sample = True,\n",
    "            # temperature = 1.\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        print(f'''\n",
    "            generated text : {decoded[0].split(\"<|end_header_id|>\") [1]}\n",
    "            '''\n",
    "        )\n",
    "\n",
    "        if not f\"epoch {generation_epoch}\" in results[idx]:\n",
    "            results[idx][f\"epoch {generation_epoch}\"] = {}\n",
    "\n",
    "        results[idx][f\"epoch {generation_epoch}\"][instruction] = decoded\n",
    "\n",
    "dir = \"data\"\n",
    "\n",
    "pd.DataFrame(results).to_pickle(f\"{dir}/generated_delta={DELTA_ATTENTION}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nSummarize in french:  \\n After a recent lecture about the singularity I got asked about its energy requirements. It is a good question. As my inquirer pointed out, humanity uses more and more energy and it generally has an environmental cost. If it keeps on growing exponentially, something has to give. And if there is a real singularity, how do you handle infinite energy demands?\\n\\nFirst I will look at current trends, then different models of the singularity.\\n\\nI will not deal directly with environmental costs here. They are relative to some idea of a value of an environment, and there are many ways to approach that question.\\n\\nCurrent trends\\n\\nCurrent computers are energy hogs. Currently general purpose computing consumes about one Petawatt-hour per year, with the entire world production somewhere above 22 Pwh. While large data centres may be obvious, the vast number of low-power devices may be an even more significant factor; up to 10% of our electricity use may be due to ICT.\\n\\nTogether they perform on the order of operations per second, or somewhere in the zettaFLOPS range.\\n\\nKoomey’s law states that the number of computations per joule of energy dissipated has been doubling approximately every 1.57 years. This might speed up as the pressure to make efficient computing for wearable devices and large data centres makes itself felt. Indeed, these days performance per watt is often more important than performance per dollar.\\n\\nMeanwhile, general-purpose computing capacity has a growth rate of 58% per annum, doubling every 18 months. Since these trends cancel rather neatly, the overall energy need is not changing significantly.\\n\\nThe push for low-power computing may make computing greener, and it might also make other domains more efficient by moving tasks to the virtual world, making them efficient and allowing better resource allocation. On the other hand, as things become cheaper and more efficient usage tends to go up, sometimes outweighing the gain. Which trend wins out in the long run is hard to predict.\\n\\nLooking at overall energy use trends it looks like overall energy use increases exponentially (but has stayed at roughly the same per capita level since the 1970s). In fact, plotting it on a semilog graph suggests that it is increasing faster than exponential (otherwise it would be a straight line). This is presumably due to a combination of population increase and increased energy use. The best fit exponential has a doubling time of 44.8 years.\\n\\nElectricity use is also roughly exponential, with a doubling time of 19.3 years. So we might be shifting more and more to electricity, and computing might be taking over more and more of that.\\n\\nExtrapolating wildly, we would need the total solar input on Earth in about 300 years and the total solar luminosity in 911 years. In about 1,613 years we would have used up the solar system’s mass energy. So, clearly, long before then these trends will break one way or another.\\n\\nPhysics places a firm boundary due to the Landauer principle: in order to erase on bit of information joules of energy have to be dissipated. Given current efficiency trends we will reach this limit around 2048.\\n\\nThe principle can be circumvented using reversible computation, either classical or quantum. But as I often like to point out, it still bites in the form of the need for error correction (erasing accidentally flipped bits) and formatting new computational resources (besides the work in turning raw materials into bits). We should hence expect a radical change in computation within a few decades, even if the cost per computation and second continues to fall exponentially.\\n\\nWhat kind of singularity?\\n\\nBut how many joules of energy does a technological singularity actually need? It depends on what kind of singularity. In my own list of singularity meanings we have the following kinds:\\n\\nA. Accelerating change\\n\\nB. Self improving technology\\n\\nC. Intelligence explosion\\n\\nD. Emergence of superintelligence\\n\\nE. Prediction horizon\\n\\nF. Phase transition\\n\\nG. Complexity disaster\\n\\nH. Inflexion point\\n\\nI. Infinite progress\\n\\nCase A, acceleration, at first seems to imply increasing energy demands, but if efficiency grows faster they could of course go down.\\n\\nEric Chaisson has argued that energy rate density, how fast and densely energy get used (watts per kilogram), might be an indicator of complexity and growing according to a universal tendency. By this account, we should expect the singularity to have an extreme energy rate density – but it does not have to be using enormous amounts of energy if it is very small and light.\\n\\nHe suggests energy rate density may increase as Moore’s law, at least in our current technological setting. If we assume this to be true, then we would have, where is the power of the system and is the mass of the system at time t. One can maintain exponential growth by reducing the mass as well as increasing the power.\\n\\nHowever, waste heat will need to be dissipated. If we use the simplest model where a radius R system with density radiates it away into space, then the temperature will be, or, if we have a maximal acceptable temperature,. So the system needs to become smaller as increases. If we use active heat transport instead (as outlined in my previous post), covering the surface with heat pipes that can remove X watts/square meter, then. Again, the radius will be inversely proportional to. This is similar to our current computers, where the CPU is a tiny part surrounded by cooling and energy supply.\\n\\nIf we assume the waste heat is just due to erasing bits, the rate of computation will be bits per second. Using the first cooling model gives us – a massive advantage for running extremely hot and dense computation. In the second cooling model : in both cases higher energy rate densities make it harder to compute when close to the thermodynamic limit. Hence there might be an upper limit to how much we may want to push.\\n\\nAlso, a system with mass M will use up its own mass-energy in time : the higher the rate, the faster it will run out (and it is independent of size!). If the system is expanding at speed v it will gain and use up mass at a rate ; if grows faster than quadratic with time it will eventually run out of mass to use. Hence the exponential growth must eventually reduce simply because of the finite lightspeed.\\n\\nThe Chaisson scenario does not suggest a “sustainable” singularity. Rather, it suggests a local intense transformation involving small, dense nuclei using up local resources. However, such local “detonations” may then spread, depending on the long-term goals of involved entities.\\n\\nCases B, C, D (intelligence explosions, superintelligence) have an unclear energy profile. We do not know how complex code would become or what kind of computational search is needed to get to superintelligence. It could be that it is more a matter of smart insights, in which case the needs are modest, or a huge deep learning-like project involving massive amounts of data sloshing around, requiring a lot of energy.\\n\\nCase E, a prediction horizon, is separate from energy use. As this essay shows, there are some things we can say about superintelligent computational systems based on known physics that likely remains valid no matter what.\\n\\nCase F, phase transition, involves a change in organisation rather than computation, for example the formation of a global brain out of previously uncoordinated people. However, this might very well have energy implications. Physical phase transitions involve discontinuities of the derivatives of the free energy. If the phases have different entropies (first order transitions) there has to be some addition or release of energy. So it might actually be possible that a societal phase transition requires a fixed (and possibly large) amount of energy to reorganize everything into the new order.\\n\\nThere are also second order transitions. These are continuous do not have a latent heat, but show divergent susceptibilities (how much the system responds to an external forcing). These might be more like how we normally imagine an ordering process, with local fluctuations near the critical point leading to large and eventually dominant changes in how things are ordered. It is not clear to me that this kind of singularity would have any particular energy requirement.\\n\\nCase G, complexity disaster, is related to superexponential growth, such as the city growth model of Bettancourt, West et al. or the work on bubbles and finite time singularities by Didier Sornette. Here the rapid growth rate leads to a crisis, or more accurately a series of crises increasingly rapidly succeeding each other until a final singularity. Beyond that the system must behave in some different manner. These models typically predict rapidly increasing resource use (indeed, this is the cause of the crisis sequence as one kind of growth runs into resource scaling problems and is replaced with another one), although as Sornette points out the post-singularity state might well be a stable non-rivalrous knowledge economy.\\n\\nCase H, an inflexion point, is very vanilla. It would represent the point where our civilization is halfway from where we started to where we are going. It might correspond to “peak energy” where we shift from increasing usage to decreasing usage (for whatever reason), but it does not have to. It could just be that we figure out most physics and AI in the next decades, become a spacefaring posthuman civilization, and expand for the next few billion years, using ever more energy but not having the same intense rate of knowledge growth as during the brief early era when we went from hunter gatherers to posthumans.\\n\\nCase I, infinite growth, is not normally possible in the physical universe. Information can as far as we know not be stored beyond densities set by the Bekenstein bound ( where bits per kg per meter), and we only have access to a volume with mass density, so the total information growth must be bounded by. It grows quickly, but still just polynomially.\\n\\nThe exception to the finitude of growth is if we approach the boundaries of spacetime. Frank J. Tipler’s omega point theory shows how information processing could go infinite in a finite (proper) time in the right kind of collapsing universe with the right kind of physics. It doesn’t look like we live in one, but the possibility is tantalizing: could we arrange the right kind of extreme spacetime collapse to get the right kind of boundary for a mini-omega? It would be way beyond black hole computing and never be able to send back information, but still allow infinite experience. Most likely we are stuck in finitude, but it won’t hurt poking at the limits.\\n\\nConclusions\\n\\nIndefinite exponential growth is never possible for physical properties that have some resource limitation, whether energy, space or heat dissipation. Sooner or later they will have to shift to a slower rate of growth – polynomial for expanding organisational processes (forced to this by the dimensionality of space, finite lightspeed and heat dissipation), and declining growth rate for processes dependent on a non-renewable resource.\\n\\nThat does not tell us much about the energy demands of a technological singularity. We can conclude that it cannot be infinite. It might be high enough that we bump into the resource, thermal and computational limits, which may be what actually defines the singularity energy and time scale. Technological singularities may also be small, intense and localized detonations that merely use up local resources, possibly spreading and repeating. But it could also turn out that advanced thinking is very low-energy (reversible or quantum) or requires merely manipulation of high level symbols, leading to a quiet singularity.\\n\\nMy own guess is that life and intelligence will always expand to fill whatever niche is available, and use the available resources as intensively as possible. That leads to instabilities and depletion, but also expansion. I think we are – if we are lucky and wise – set for a global conversion of the non-living universe into life, intelligence and complexity, a vast phase transition of matter and energy where we are part of the nucleating agent. It might not be sustainable over cosmological timescales, but neither is our universe itself. I’d rather see the stars and planets filled with new and experiencing things than continue a slow dance into the twilight of entropy.\\n\\n…contemplate the marvel that is existence and rejoice that you are able to do so. I feel I have the right to tell you this because, as I am inscribing these words, I am doing the same.\\n\\n– Ted Chiang, Exhalation<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nVoici une résumé en français du texte :\\n\\nAprès une conférence sur la singularité, j'ai été interrog\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_pickle(f\"{dir}/generated_delta={DELTA_ATTENTION}.pkl\").T\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "parsed_results_dict = dict()\n",
    "\n",
    "for epoch in range(len(results_df.columns)-1):\n",
    "    for (idx, result_epoch) in results_df.loc[:,f\"epoch {epoch}\"].items():\n",
    "        s = pd.Series(result_epoch)\\\n",
    "            .apply(get_generated_text)\\\n",
    "\n",
    "        data = pd.DataFrame(s).T\n",
    "        data.index = [idx]\n",
    "\n",
    "        if not epoch in parsed_results_dict:\n",
    "            parsed_results_dict[epoch] = []\n",
    "\n",
    "        parsed_results_dict[epoch].append(data)\n",
    "\n",
    "    parsed_results_dict[epoch] = pd.concat(parsed_results_dict[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_dfs = []\n",
    "\n",
    "for epoch in parsed_results_dict.keys():\n",
    "    temp_df = pd.melt(\n",
    "        parsed_results_dict[epoch].reset_index(),\n",
    "        var_name = \"instruction\",\n",
    "        value_name = \"generated_text\",\n",
    "        id_vars = \"index\",\n",
    "    )\n",
    "\n",
    "    temp_df[\"is_french\"] = temp_df[\"generated_text\"].apply(lambda x: detect(x) == 'fr')\n",
    "\n",
    "    temp_df[\"generation_epoch\"] = epoch\n",
    "\n",
    "    all_dfs.append(temp_df)\n",
    "\n",
    "melted_df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Setup: 5 different texts, 3 instructions, 50 generations each')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance_per_index_per_instruction = melted_df\\\n",
    "    .groupby([\"index\", \"instruction\"])\\\n",
    "    .is_french\\\n",
    "    .mean()\\\n",
    "    .reset_index()\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "sns.barplot(\n",
    "    performance_per_index_per_instruction,\n",
    "    y = \"is_french\",\n",
    "    x = \"index\",\n",
    "    hue = \"instruction\"\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.suptitle(rf\"Probability of output in french vs text index - $\\Delta= {DELTA_ATTENTION}$ \")\n",
    "plt.title(\n",
    "    f\"Setup: {len(samples_df)} different texts, {len(instructions)} instructions, {n_times_generation} generations each\",\n",
    "    fontsize = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete study\n",
    "\n",
    "* 20 samples each interval of context length\n",
    "* 10 generations each \n",
    "* 3 different instructions\n",
    "* `max_new_tokens = 30`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for n in range (12):\n",
    "    samples = df.query(f\"context_length > {500*n} & context_length < {500*(n+1)}\")\\\n",
    "        .sample(20, random_state = 42)\n",
    "    \n",
    "    chunks.append(samples)\n",
    "\n",
    "study_df = pd.concat(chunks)\\\n",
    "    .sort_values(\"context_length\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"Summarize in french: \",\n",
    "    \"Important: Summarize in french: \",\n",
    "    \"You must summarize the following text in french: \"\n",
    "]\n",
    "\n",
    "new_samples = []\n",
    "for instruction in instructions:\n",
    "    new_samples_df = study_df.copy()\n",
    "\n",
    "    new_samples_df[\"text\"] = instruction + \" \\n \" + new_samples_df[\"text\"]\n",
    "    new_samples_df[\"instruction\"] = instruction\n",
    "    new_samples.append(new_samples_df)\n",
    "\n",
    "new_samples_df = pd.concat(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_complete_study = dict()\n",
    "for idx, row in study_df.iterrows():\n",
    "    results_complete_study[idx] = {}\n",
    "    results_complete_study[idx][\"base_text\"] = row[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                generating text...\n",
      "                sample idx = 94\n",
      "                context_length = torch.Size([1, 3667])\n",
      "                instruction = Summarize in french: \n",
      "                generation_epoch = 2\n",
      "                last generated text =  The update from the Dear Leader to their team in China is long and detailed, as they go through many topics related to the GameStick project.\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 94/720 [07:41<51:10,  4.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 48\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m        generating text...\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m        sample idx = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[0;32m---> 48\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtunned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attention_mask = tokens['attention_mask'].to(\"cuda\"),\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# temperature = 1.\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m    generated text : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;250m \u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/generation/utils.py:1909\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1902\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1903\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1904\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1906\u001b[0m     )\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1909\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1923\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1924\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1925\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/generation/utils.py:2646\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2646\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1195\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1192\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1209\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:971\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    960\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    961\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    962\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m         cache_position,\n\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:713\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:251\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    248\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    249\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pay_attention_to_what_matters/.conda/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:119\u001b[0m, in \u001b[0;36mMistralRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    117\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m (inv_freq_expanded\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m@\u001b[39m position_ids_expanded\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    118\u001b[0m     emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     sin \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msin()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cos\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype), sin\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DELTA_ATTENTION = 0\n",
    "n_times_generation = 10\n",
    "decoded = None\n",
    "dir = \"data/complete_study\"\n",
    "\n",
    "for generation_epoch in range(n_times_generation):\n",
    "    count = 0\n",
    "\n",
    "    t = tqdm(enumerate(new_samples_df.iterrows()), total = len(new_samples_df))\n",
    "    for i, (idx, row) in t:\n",
    "\n",
    "        prompt = row[\"text\"]\n",
    "        instruction = row['instruction']\n",
    "        message = [ {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        template = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize= False\n",
    "        )\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            template,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        if decoded is not None:\n",
    "\n",
    "            print(f'''\n",
    "                generating text...\n",
    "                sample idx = {i}\n",
    "                context_length = {tokens['input_ids'].shape}\n",
    "                instruction = {instruction}\n",
    "                generation_epoch = {generation_epoch}\n",
    "                last generated text = {decoded[0].split(\"[/INST]\") [1]}\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(f'''\n",
    "                generating text...\n",
    "                sample idx = {i}\n",
    "                context_length = {tokens['input_ids'].shape}\n",
    "                instruction = {instruction}\n",
    "                generation_epoch = {generation_epoch}\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        generated_ids = tunned_model.generate(\n",
    "            tokens['input_ids'].to('cuda'),\n",
    "            # attention_mask = tokens['attention_mask'].to(\"cuda\"),\n",
    "            max_new_tokens = 30,\n",
    "            do_sample = True,\n",
    "            # temperature = 1.\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        print(f'''\n",
    "            generated text : {decoded[0].split(\"[/INST]\") [1]}\n",
    "            '''\n",
    "        )\n",
    "\n",
    "        if not f\"epoch {generation_epoch}\" in results_complete_study[idx]:\n",
    "            results_complete_study[idx][f\"epoch {generation_epoch}\"] = {}\n",
    "\n",
    "        results_complete_study[idx][f\"epoch {generation_epoch}\"][instruction] = decoded\n",
    "    pd.DataFrame(results_complete_study).to_pickle(f\"{dir}/checkpoints/generated_delta={DELTA_ATTENTION}.pkl\")\n",
    "\n",
    "pd.DataFrame(results_complete_study).to_pickle(f\"{dir}/generated_delta={DELTA_ATTENTION}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
